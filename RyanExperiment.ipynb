{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79c17f-1643-4240-9e34-acfeb8ca3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original GAN and face crop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function for weight initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_size=32, num_classes=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.label_embeddings = nn.Embedding(num_classes, self.emb_size)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),  # Output: (64, 32, 32)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),  # Output: (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),  # Output: (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),  # Output: (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Calculate flattened size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 64, 64)  # Match input dimensions\n",
    "            dummy_output = self.model(dummy_input)\n",
    "            self.flattened_size = dummy_output.size(1)  # Should be 512*4*4=8192\n",
    "            print(f\"Discriminator Flattened Size: {self.flattened_size}\")  # Debug statement\n",
    "\n",
    "        # Adjust input size of Linear layer\n",
    "        self.model2 = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size + self.emb_size, 100),  # Should be 8192 + 32 = 8224\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        x = self.model(input)  # Convolutional and flattening layers\n",
    "        y = self.label_embeddings(labels)  # Label embeddings\n",
    "        combined = torch.cat([x, y], dim=1)  # Concatenate\n",
    "        print(f\"Combined Shape: {combined.shape}\")  # Debug statement\n",
    "        return self.model2(combined)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, emb_size=32, num_classes=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.label_embeddings = nn.Embedding(num_classes, self.emb_size)\n",
    "        self.model = nn.Sequential(\n",
    "            # 1. From latent vector + label embedding\n",
    "            nn.ConvTranspose2d(100 + self.emb_size, 1024, 4, 1, 0, bias=False),  # (132, 1, 1) -> (1024, 4, 4)\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 2. Upsample to 8x8\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),  # (512, 8, 8)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 3. Upsample to 16x16\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),  # (256, 16, 16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 4. Upsample to 32x32\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 5. Upsample to 64x64\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),  # (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 6. Convert to 3 channels without upsampling\n",
    "            nn.Conv2d(64, 3, 3, 1, 1, bias=False),  # (3, 64, 64)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input_noise, labels):\n",
    "        label_embeddings = self.label_embeddings(labels).view(labels.size(0), self.emb_size, 1, 1)\n",
    "        input = torch.cat([input_noise, label_embeddings], dim=1)\n",
    "        return self.model(input)\n",
    "\n",
    "# Function to create a new experiment directory with an incremental index\n",
    "def create_experiment_dir(base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        return os.path.join(base_dir, \"experiment_1\")\n",
    "    existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"experiment_\")]\n",
    "    if not existing:\n",
    "        return os.path.join(base_dir, \"experiment_1\")\n",
    "    indices = [int(d.split(\"_\")[1]) for d in existing if d.split(\"_\")[1].isdigit()]\n",
    "    next_index = max(indices) + 1 if indices else 1\n",
    "    return os.path.join(base_dir, f\"experiment_{next_index}\")\n",
    "\n",
    "# Function to crop faces from images\n",
    "def crop_faces(input_dir, output_dir, face_cascade_path='haarcascade_frontalface_default.xml'):\n",
    "    \"\"\"\n",
    "    Detects and crops faces from images in the input directory and saves them to the output directory,\n",
    "    maintaining class subdirectories.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing input images organized in class subdirectories.\n",
    "        output_dir (str): Path to the directory where cropped face images will be saved, maintaining class subdirectories.\n",
    "        face_cascade_path (str): Path to the Haar Cascade XML file for face detection.\n",
    "    \"\"\"\n",
    "    # Initialize face cascade\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + face_cascade_path)\n",
    "    if face_cascade.empty():\n",
    "        raise IOError(\"Cannot load Haar cascade xml file for face detection.\")\n",
    "\n",
    "    # Get list of class directories\n",
    "    class_dirs = [d for d in Path(input_dir).iterdir() if d.is_dir()]\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        class_name = class_dir.name\n",
    "        output_class_dir = Path(output_dir) / class_name\n",
    "        output_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get list of image paths in the class directory\n",
    "        image_paths = sorted(class_dir.glob(\"*.jpg\"))  # Adjust extension if needed\n",
    "        \n",
    "        for i, img_path in enumerate(tqdm(image_paths, desc=f\"Cropping Faces in {class_name}\")):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                print(f\"Warning: Unable to read image {img_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "            if len(faces) == 0:\n",
    "                print(f\"No face detected in image {img_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Save only the first detected face\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face = img[y:y+h, x:x+w]\n",
    "\n",
    "            # Convert BGR to RGB before saving\n",
    "            face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face_pil = Image.fromarray(face_rgb)\n",
    "\n",
    "            # Resize the face to a consistent size before saving\n",
    "            face_pil = face_pil.resize((64, 64), Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "            # Save the cropped face\n",
    "            save_path = output_class_dir / f\"{class_name}_{i}.jpg\"\n",
    "            face_pil.save(save_path)\n",
    "\n",
    "# Preprocess images by cropping faces\n",
    "input_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/structured_images\"\n",
    "cropped_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/cropped_faces\"\n",
    "\n",
    "print(\"Starting face cropping...\")\n",
    "crop_faces(input_dir, cropped_dir)\n",
    "print(\"Face cropping completed.\")\n",
    "\n",
    "# Define dataset and transforms without CenterCrop\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Images are already cropped; ensure consistent size\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.RandomRotation(10),      # Data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Corrected for 3 channels\n",
    "])\n",
    "\n",
    "# Update dataset path to use cropped faces\n",
    "dataset_path = \"C:/Users/ryan9/Documents/CV/Team Project 3/cropped_faces\"\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets (e.g., 80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize models, optimizers, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(emb_size=32, num_classes=4).to(device)\n",
    "discriminator = Discriminator(emb_size=32, num_classes=4).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Unified lower learning rate for both Generator and Discriminator\n",
    "learning_rate = 0.0001\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Paths for saving models and examples with indexed subfolders\n",
    "base_model_save_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/saved_models\"\n",
    "base_example_save_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/examples\"\n",
    "\n",
    "# Create new experiment directories\n",
    "model_save_dir = create_experiment_dir(base_model_save_dir)\n",
    "example_save_dir = create_experiment_dir(base_example_save_dir)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "os.makedirs(example_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Models will be saved to: {model_save_dir}\")\n",
    "print(f\"Examples will be saved to: {example_save_dir}\")\n",
    "\n",
    "# Dummy Forward Pass Test\n",
    "print(\"Performing dummy forward pass to verify model dimensions...\")\n",
    "dummy_noise = torch.randn(128, 100, 1, 1, device=device)\n",
    "dummy_labels = torch.randint(0, 4, (128,), device=device)\n",
    "fake_imgs = generator(dummy_noise, dummy_labels)\n",
    "output = discriminator(fake_imgs.detach(), dummy_labels)\n",
    "print(f\"Discriminator Output Shape: {output.shape}\")  # Should be (128, 1)\n",
    "\n",
    "# Training loop with increased epochs and validation\n",
    "epochs = 100  # Increased number of epochs\n",
    "latent_dim = 100\n",
    "noise_std = 0.1  # Standard deviation for added Gaussian noise\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_loss_epoch = 0\n",
    "    d_loss_epoch = 0\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        # Apply label smoothing\n",
    "        valid = torch.ones((batch_size, 1), device=device) * 0.9  # Smooth labels for real images\n",
    "        fake = torch.zeros((batch_size, 1), device=device)        # Keep fake labels as 0\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Add Gaussian noise to real images\n",
    "        real_imgs_noisy = imgs + noise_std * torch.randn_like(imgs)\n",
    "        real_imgs_noisy = torch.clamp(real_imgs_noisy, -1, 1)  # Ensure images are still in [-1,1]\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "        gen_labels = torch.randint(0, 4, (batch_size,), device=device)  # Update to 4 classes\n",
    "        fake_imgs = generator(noise, gen_labels)\n",
    "\n",
    "        real_loss = criterion(discriminator(real_imgs_noisy, labels), valid)\n",
    "        fake_loss = criterion(discriminator(fake_imgs.detach(), gen_labels), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Add Gaussian noise to fake images\n",
    "        fake_imgs_noisy = fake_imgs + noise_std * torch.randn_like(fake_imgs)\n",
    "        fake_imgs_noisy = torch.clamp(fake_imgs_noisy, -1, 1)  # Ensure images are still in [-1,1]\n",
    "\n",
    "        g_loss = criterion(discriminator(fake_imgs_noisy, gen_labels), valid)  # Target generator outputs as real\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        g_loss_epoch += g_loss.item()\n",
    "        d_loss_epoch += d_loss.item()\n",
    "\n",
    "    # Log and print losses for the epoch\n",
    "    g_loss_epoch /= len(train_loader)\n",
    "    d_loss_epoch /= len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Generator Loss: {g_loss_epoch:.4f}, Discriminator Loss: {d_loss_epoch:.4f}\")\n",
    "\n",
    "    # Save models after each epoch\n",
    "    torch.save(generator.state_dict(), os.path.join(model_save_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(model_save_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "    # Validation step\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    val_d_loss = 0\n",
    "    val_g_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            # Real images\n",
    "            valid = torch.ones((batch_size, 1), device=device) * 0.9\n",
    "            # Fake images\n",
    "            fake = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "            # Add noise to real images\n",
    "            real_imgs_noisy = imgs + noise_std * torch.randn_like(imgs)\n",
    "            real_imgs_noisy = torch.clamp(real_imgs_noisy, -1, 1)\n",
    "\n",
    "            # Discriminator loss on real images\n",
    "            real_loss = criterion(discriminator(real_imgs_noisy, labels), valid)\n",
    "\n",
    "            # Generate fake images\n",
    "            noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "            gen_labels = torch.randint(0, 4, (batch_size,), device=device)  # Update to 4 classes\n",
    "            fake_imgs = generator(noise, gen_labels)\n",
    "\n",
    "            # Add noise to fake images\n",
    "            fake_imgs_noisy = fake_imgs + noise_std * torch.randn_like(fake_imgs)\n",
    "            fake_imgs_noisy = torch.clamp(fake_imgs_noisy, -1, 1)\n",
    "\n",
    "            # Discriminator loss on fake images\n",
    "            fake_loss = criterion(discriminator(fake_imgs_noisy, gen_labels), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            val_d_loss += d_loss.item()\n",
    "\n",
    "            # Generator loss\n",
    "            g_loss = criterion(discriminator(fake_imgs_noisy, gen_labels), valid)\n",
    "            val_g_loss += g_loss.item()\n",
    "\n",
    "    val_d_loss /= len(val_loader)\n",
    "    val_g_loss /= len(val_loader)\n",
    "    print(f\"Validation - Generator Loss: {val_g_loss:.4f}, Discriminator Loss: {val_d_loss:.4f}\")\n",
    "\n",
    "    # Save generated examples after each epoch\n",
    "    with torch.no_grad():\n",
    "        fixed_noise = torch.randn(4, latent_dim, 1, 1, device=device)  # One per class\n",
    "        fixed_labels = torch.arange(0, 4, device=device)  # 0,1,2,3\n",
    "        generated_imgs = generator(fixed_noise, fixed_labels)\n",
    "        generated_imgs = (generated_imgs + 1) / 2  # Rescale to [0, 1]\n",
    "\n",
    "        grid = torchvision.utils.make_grid(generated_imgs.cpu(), nrow=4)\n",
    "        plt.figure(figsize=(16, 4))\n",
    "        plt.imshow(grid.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(example_save_dir, f\"epoch_{epoch+1}.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0363cc-5b9e-4a93-ae24-e178021913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cGAN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "\n",
    "# ------------------------------\n",
    "# Helper Classes and Functions\n",
    "# ------------------------------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "def preprocess_for_fid(images):\n",
    "    images = (images + 1) * 127.5  # Scale from [-1, 1] to [0, 255]\n",
    "    images = images.clamp(0, 255).byte()  # Clamp and convert to uint8\n",
    "    return images\n",
    "    \n",
    "# Helper function for weight initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Function to create a new experiment directory with an incremental index\n",
    "def create_experiment_dir(base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        return os.path.join(base_dir, \"experiment_1\")\n",
    "    existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"experiment_\")]\n",
    "    if not existing:\n",
    "        return os.path.join(base_dir, \"experiment_1\")\n",
    "    indices = [int(d.split(\"_\")[1]) for d in existing if d.split(\"_\")[1].isdigit()]\n",
    "    next_index = max(indices) + 1 if indices else 1\n",
    "    return os.path.join(base_dir, f\"experiment_{next_index}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Updated Generator\n",
    "# ------------------------------\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, emb_size=32, num_classes=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_classes = num_classes\n",
    "        self.label_embeddings = nn.Embedding(num_classes, emb_size)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # Output: (64, 32, 32)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # Output: (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # Output: (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # Output: (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(512),\n",
    "            ResidualBlock(512),\n",
    "            ResidualBlock(512),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512 + emb_size, 256, 4, 2, 1),  # Output: (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # Output: (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # Output: (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),    # Output: (3, 64, 64)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input_images, target_labels):\n",
    "        x = self.encoder(input_images)  # (batch_size, 512, 4, 4)\n",
    "        x = self.residual_blocks(x)     # (batch_size, 512, 4, 4)\n",
    "        \n",
    "        # Embed labels and expand to match spatial dimensions\n",
    "        label_embeddings = self.label_embeddings(target_labels).unsqueeze(2).unsqueeze(3)  # (batch_size, emb_size, 1, 1)\n",
    "        label_embeddings = label_embeddings.repeat(1, 1, x.size(2), x.size(3))             # (batch_size, emb_size, 4, 4)\n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([x, label_embeddings], dim=1)  # (batch_size, 512 + emb_size, 4, 4)\n",
    "        output_images = self.decoder(x)             # (batch_size, 3, 64, 64)\n",
    "        return output_images\n",
    "\n",
    "# ------------------------------\n",
    "# Updated Discriminator\n",
    "# ------------------------------\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_size=32, num_classes=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_classes = num_classes\n",
    "        self.label_embeddings = nn.Embedding(num_classes, emb_size)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3 + emb_size, 64, 4, 2, 1, bias=False),  # (64, 32, 32)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),           # (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),          # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),          # (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4 + emb_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.aux_layer = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4 + emb_size, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input_images, labels):\n",
    "        # Embed labels and expand to match spatial dimensions\n",
    "        label_embeddings = self.label_embeddings(labels).unsqueeze(2).unsqueeze(3)  # (batch_size, emb_size, 1, 1)\n",
    "        label_embeddings = label_embeddings.repeat(1, 1, input_images.size(2), input_images.size(3))  # (batch_size, emb_size, H, W)\n",
    "        \n",
    "        # Concatenate image and label embeddings\n",
    "        x = torch.cat([input_images, label_embeddings], dim=1)  # (batch_size, 3 + emb_size, 64, 64)\n",
    "        x = self.model(x)  # (batch_size, 512 * 4 * 4)\n",
    "        \n",
    "        # Concatenate with label embeddings\n",
    "        label_embeddings_flat = self.label_embeddings(labels)  # (batch_size, emb_size)\n",
    "        x = torch.cat([x, label_embeddings_flat], dim=1)      # (batch_size, 512*4*4 + emb_size)\n",
    "        \n",
    "        validity = self.adv_layer(x)  # (batch_size, 1)\n",
    "        label_pred = self.aux_layer(x)  # (batch_size, num_classes)\n",
    "        return validity, label_pred\n",
    "\n",
    "# ------------------------------\n",
    "# Perceptual Loss\n",
    "# ------------------------------\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.vgg_layers = nn.Sequential(*[vgg[i] for i in range(35)]).eval()\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.vgg_layers.to(device)  # Move VGG layers to the specified device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Normalize input for VGG\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(x.device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(x.device).view(1, 3, 1, 1)\n",
    "        x_norm = (x - mean) / std\n",
    "        y_norm = (y - mean) / std\n",
    "        x_features = self.vgg_layers(x_norm)\n",
    "        y_features = self.vgg_layers(y_norm)\n",
    "        return self.criterion(x_features, y_features)\n",
    "\n",
    "# ------------------------------\n",
    "# Minibatch Discrimination (Removed)\n",
    "# ------------------------------\n",
    "# The MinibatchDiscrimination layer has been removed as it's not commonly used\n",
    "# and can complicate training. The updated Discriminator uses Auxiliary Classifier (ACGAN) approach.\n",
    "\n",
    "# ------------------------------\n",
    "# Data Preparation\n",
    "# ------------------------------\n",
    "\n",
    "# Define dataset and transforms without CenterCrop\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Ensure consistent size\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.RandomRotation(10),      # Data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Update dataset path to use cropped faces\n",
    "dataset_path = \"C:/Users/ryan9/Documents/CV/Team Project 3/cropped_faces\"\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets (e.g., 80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "# To reduce verbosity, set num_workers to 0 if encountering issues with multiple workers\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Initialize Models, Optimizers, and Loss Functions\n",
    "# ------------------------------\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    # Limit the GPU memory usage to 80%\n",
    "    torch.cuda.set_per_process_memory_fraction(0.8, device.index)\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(emb_size=32, num_classes=4).to(device)\n",
    "discriminator = Discriminator(emb_size=32, num_classes=4).to(device)\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_aux = nn.CrossEntropyLoss()\n",
    "criterion_perceptual = PerceptualLoss(device).to(device)\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "learning_rate = 0.0001\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation Metrics\n",
    "# ------------------------------\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "inception_score = InceptionScore().to(device)\n",
    "\n",
    "# ------------------------------\n",
    "# Paths for Saving Models and Examples\n",
    "# ------------------------------\n",
    "\n",
    "base_model_save_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/saved_models\"\n",
    "base_example_save_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/examples\"\n",
    "base_fid_save_dir = \"C:/Users/ryan9/Documents/CV/Team Project 3/fid_scores\"\n",
    "\n",
    "# Create new experiment directories\n",
    "model_save_dir = create_experiment_dir(base_model_save_dir)\n",
    "example_save_dir = create_experiment_dir(base_example_save_dir)\n",
    "fid_save_dir = create_experiment_dir(base_fid_save_dir)\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "os.makedirs(example_save_dir, exist_ok=True)\n",
    "os.makedirs(fid_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Models will be saved to: {model_save_dir}\")\n",
    "print(f\"Examples will be saved to: {example_save_dir}\")\n",
    "print(f\"FID scores will be saved to: {fid_save_dir}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Dummy Forward Pass Test\n",
    "# ------------------------------\n",
    "print(\"Performing dummy forward pass to verify model dimensions...\")\n",
    "dummy_noise = torch.randn(128, 100, 1, 1, device=device)  # Not used in updated Generator\n",
    "dummy_labels = torch.randint(0, 4, (128,), device=device)\n",
    "# Since Generator now requires input images, perform accordingly\n",
    "dummy_input = torch.randn(128, 3, 64, 64, device=device)\n",
    "fake_imgs = generator(dummy_input, dummy_labels)\n",
    "validity, label_pred = discriminator(fake_imgs.detach(), dummy_labels)\n",
    "print(f\"Discriminator Output Shape (Validity): {validity.shape}\")    # Should be (128, 1)\n",
    "print(f\"Discriminator Output Shape (Label): {label_pred.shape}\")    # Should be (128, num_classes)\n",
    "\n",
    "# ------------------------------\n",
    "# Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "epochs = 100  # Number of epochs\n",
    "latent_dim = 100  # Not used in updated Generator\n",
    "noise_std = 0.1  # Standard deviation for Gaussian noise\n",
    "alpha = 0.5  # Weight for perceptual loss\n",
    "beta = 0.5   # Weight for L1 loss (if used)\n",
    "\n",
    "# Fixed noise and labels for generating consistent images\n",
    "fixed_input = torch.randn(4, 3, 64, 64, device=device)  # Random input images\n",
    "fixed_labels = torch.arange(0, 4, device=device)        # One label per class\n",
    "\n",
    "# Initialize tqdm for epoch progress\n",
    "epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\", dynamic_ncols=True)\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_loss_epoch = 0\n",
    "    d_loss_epoch = 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        real_valid = torch.ones((batch_size, 1), device=device) * 0.9  # Label smoothing\n",
    "        real_imgs_noisy = imgs + noise_std * torch.randn_like(imgs)\n",
    "        real_imgs_noisy = torch.clamp(real_imgs_noisy, -1, 1)  # Keep within [-1,1]\n",
    "\n",
    "        # Discriminator forward pass on real images\n",
    "        real_pred, real_aux = discriminator(real_imgs_noisy, labels)\n",
    "        d_real_loss = criterion_GAN(real_pred, real_valid)\n",
    "        d_real_aux_loss = criterion_aux(real_aux, labels)\n",
    "\n",
    "        # Generate fake images\n",
    "        target_labels = torch.randint(0, 4, (batch_size,), device=device)\n",
    "        fake_imgs = generator(imgs, target_labels)\n",
    "        fake_imgs_noisy = fake_imgs + noise_std * torch.randn_like(fake_imgs)\n",
    "        fake_imgs_noisy = torch.clamp(fake_imgs_noisy, -1, 1)\n",
    "\n",
    "        # Discriminator forward pass on fake images\n",
    "        fake_valid = torch.zeros((batch_size, 1), device=device)\n",
    "        fake_pred, fake_aux = discriminator(fake_imgs_noisy.detach(), target_labels)\n",
    "        d_fake_loss = criterion_GAN(fake_pred, fake_valid)\n",
    "        d_fake_aux_loss = criterion_aux(fake_aux, target_labels)\n",
    "\n",
    "        # Total Discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) + (d_real_aux_loss + d_fake_aux_loss)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_imgs = generator(imgs, target_labels)\n",
    "        fake_imgs_noisy = fake_imgs + noise_std * torch.randn_like(fake_imgs)\n",
    "        fake_imgs_noisy = torch.clamp(fake_imgs_noisy, -1, 1)\n",
    "\n",
    "        # Discriminator evaluates the fake images\n",
    "        pred_fake, aux_fake = discriminator(fake_imgs_noisy, target_labels)\n",
    "\n",
    "        # Generator adversarial loss\n",
    "        g_adv = criterion_GAN(pred_fake, real_valid)  # Wants discriminator to believe it's real\n",
    "\n",
    "        # Auxiliary loss\n",
    "        g_aux = criterion_aux(aux_fake, target_labels)\n",
    "\n",
    "        # Perceptual loss\n",
    "        g_perc = criterion_perceptual(fake_imgs, imgs)\n",
    "\n",
    "        # L1 loss (optional, can help with image quality)\n",
    "        g_L1 = criterion_L1(fake_imgs, imgs)\n",
    "\n",
    "        # Total Generator loss\n",
    "        g_loss = g_adv + alpha * g_perc + beta * g_L1 + g_aux\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Accumulate epoch losses\n",
    "        g_loss_epoch += g_loss.item()\n",
    "        d_loss_epoch += d_loss.item()\n",
    "\n",
    "    # Calculate average losses for the epoch\n",
    "    g_loss_epoch /= len(train_loader)\n",
    "    d_loss_epoch /= len(train_loader)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Validation and Evaluation\n",
    "    # ---------------------\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    val_d_loss = 0\n",
    "    val_g_loss = 0\n",
    "    fid.reset()\n",
    "    inception_score.reset()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.size(0)\n",
    "    \n",
    "        # Preprocess real images for FID\n",
    "        real_imgs_uint8 = preprocess_for_fid(imgs)\n",
    "    \n",
    "        # Generate fake images\n",
    "        target_labels = torch.randint(0, 4, (batch_size,), device=device)\n",
    "        fake_imgs = generator(imgs, target_labels)\n",
    "    \n",
    "        # Preprocess fake images for FID\n",
    "        fake_imgs_uint8 = preprocess_for_fid(fake_imgs)\n",
    "    \n",
    "        # Update FID\n",
    "        fid.update(fake_imgs_uint8, real=False)\n",
    "        fid.update(real_imgs_uint8, real=True)\n",
    "    \n",
    "        # **Preprocess fake images for Inception Score (ensure uint8)**\n",
    "        fake_imgs_for_is = preprocess_for_fid(fake_imgs)\n",
    "        inception_score.update(fake_imgs_for_is)\n",
    "    \n",
    "        # Discriminator loss\n",
    "        real_valid = torch.ones((batch_size, 1), device=device) * 0.9\n",
    "        real_imgs_noisy = imgs + noise_std * torch.randn_like(imgs)\n",
    "        real_imgs_noisy = torch.clamp(real_imgs_noisy, -1, 1)\n",
    "        real_pred, real_aux = discriminator(real_imgs_noisy, labels)\n",
    "        d_real_loss = criterion_GAN(real_pred, real_valid)\n",
    "        d_real_aux_loss = criterion_aux(real_aux, labels)\n",
    "    \n",
    "        fake_valid = torch.zeros((batch_size, 1), device=device)\n",
    "        fake_imgs_noisy = fake_imgs + noise_std * torch.randn_like(fake_imgs)\n",
    "        fake_imgs_noisy = torch.clamp(fake_imgs_noisy, -1, 1)\n",
    "        fake_pred, fake_aux = discriminator(fake_imgs_noisy, target_labels)\n",
    "        d_fake_loss = criterion_GAN(fake_pred, fake_valid)\n",
    "        d_fake_aux_loss = criterion_aux(fake_aux, target_labels)\n",
    "    \n",
    "        # Generator loss\n",
    "        pred_fake, aux_fake = discriminator(fake_imgs_noisy, target_labels)\n",
    "        g_adv = criterion_GAN(pred_fake, real_valid)\n",
    "        g_aux = criterion_aux(aux_fake, target_labels)\n",
    "        g_perc = criterion_perceptual(fake_imgs, imgs)\n",
    "        g_L1 = criterion_L1(fake_imgs, imgs)\n",
    "        g_loss = g_adv + alpha * g_perc + beta * g_L1 + g_aux\n",
    "    \n",
    "        # Update validation losses\n",
    "        val_d_loss += (d_real_loss + d_fake_loss + d_real_aux_loss + d_fake_aux_loss).item()\n",
    "        val_g_loss += g_loss.item()\n",
    "\n",
    "# After the loop\n",
    "val_d_loss /= len(val_loader)\n",
    "val_g_loss /= len(val_loader)\n",
    "\n",
    "# Compute FID and Inception Score\n",
    "current_fid = fid.compute().item()\n",
    "current_is, current_is_std = inception_score.compute()\n",
    "\n",
    "# Reset metrics\n",
    "fid.reset()\n",
    "inception_score.reset()\n",
    "\n",
    "# Update progress bar with epoch-level metrics\n",
    "epoch_pbar.set_postfix({\n",
    "    'G_Loss': f\"{g_loss_epoch:.4f}\",\n",
    "    'D_Loss': f\"{d_loss_epoch:.4f}\",\n",
    "    'Val_G_Loss': f\"{val_g_loss:.4f}\",\n",
    "    'Val_D_Loss': f\"{val_d_loss:.4f}\",\n",
    "    'FID': f\"{current_fid:.2f}\",\n",
    "    'IS': f\"{current_is:.2f}±{current_is_std:.2f}\"\n",
    "})\n",
    "\n",
    "# ... rest of your code remains unchanged ...\n",
    "\n",
    "# ---------------------\n",
    "#  Save Models\n",
    "# ---------------------\n",
    "torch.save(generator.state_dict(), os.path.join(model_save_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "torch.save(discriminator.state_dict(), os.path.join(model_save_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "# ---------------------\n",
    "#  Save Generated Examples\n",
    "# ---------------------\n",
    "with torch.no_grad():\n",
    "    generated_imgs = generator(fixed_input, fixed_labels)\n",
    "    generated_imgs = (generated_imgs + 1) / 2  # Rescale to [0, 1]\n",
    "\n",
    "    grid = torchvision.utils.make_grid(generated_imgs.cpu(), nrow=4)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(example_save_dir, f\"epoch_{epoch+1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------\n",
    "#  Save FID Score\n",
    "# ---------------------\n",
    "with open(os.path.join(fid_save_dir, \"fid_scores.txt\"), \"a\") as f:\n",
    "    f.write(f\"Epoch {epoch+1}: FID = {current_fid:.2f}, IS = {current_is:.2f}±{current_is_std:.2f}\\n\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# ------------------------------\n",
    "# Re-aging Function for All Categories\n",
    "# ------------------------------\n",
    "\n",
    "def re_age_faces_all_categories(generator, input_images, original_ages, save_dir, device):\n",
    "    \"\"\"\n",
    "    Re-age faces in the input images to all age categories and save with original/target age in filenames.\n",
    "\n",
    "    Args:\n",
    "        generator (nn.Module): Trained generator model.\n",
    "        input_images (torch.Tensor): Batch of input images.\n",
    "        original_ages (torch.Tensor): Original age labels for the input images.\n",
    "        save_dir (str): Directory to save re-aged images.\n",
    "        device (torch.device): Device to run computations on.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate through all examples\n",
    "        for i, (img, original_age) in enumerate(zip(input_images, original_ages)):\n",
    "            img = img.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            original_age = original_age.item()\n",
    "\n",
    "            # Generate re-aged images for all four categories\n",
    "            for target_age in range(4):\n",
    "                target_age_tensor = torch.tensor([target_age], device=device)\n",
    "\n",
    "                # Generate re-aged image\n",
    "                re_aged_img = generator(img, target_age_tensor)\n",
    "                re_aged_img = (re_aged_img + 1) / 2  # Rescale to [0, 1]\n",
    "\n",
    "                # Convert to image and save with detailed filename\n",
    "                img_np = re_aged_img.squeeze(0).cpu().permute(1, 2, 0).numpy()  # Remove batch dimension\n",
    "                img_pil = Image.fromarray((img_np * 255).astype(\"uint8\"))\n",
    "                filename = f\"image_{i}_original_age_{original_age}_target_age_{target_age}.png\"\n",
    "                img_pil.save(os.path.join(save_dir, filename))\n",
    "\n",
    "# ------------------------------\n",
    "# Test Re-aging on Dataset\n",
    "# ------------------------------\n",
    "\n",
    "# Take 20 examples from the dataset\n",
    "examples_to_process = 20\n",
    "sample_images, sample_labels = next(iter(val_loader))\n",
    "\n",
    "# Limit to 20 examples\n",
    "sample_images = sample_images[:examples_to_process]\n",
    "sample_labels = sample_labels[:examples_to_process]\n",
    "\n",
    "# Save re-aged images\n",
    "re_age_faces_all_categories(\n",
    "    generator,\n",
    "    sample_images,\n",
    "    original_ages=sample_labels,\n",
    "    save_dir=\"C:/Users/ryan9/Documents/CV/Team Project 3/re_aged_faces_all_categories\",\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
